{"cells":[{"metadata":{"id":"5Gf2nZUwSU0i"},"cell_type":"markdown","source":"# 9. Deep Learning with PyTorch\n\n이번 세션에서는 PyTorch 패키지를 사용하여 ML모델의 정의, 학습 및 예측 실습을 합니다.\n\n먼저 다음의 코드를 실행하여 필요한 라이브러리들을 import 합니다.\n"},{"metadata":{"id":"IDZKNoewSROQ","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport time","execution_count":1,"outputs":[]},{"metadata":{"id":"BWqzcCih1dW7"},"cell_type":"markdown","source":"## 9.1. Getting Started with PyTorch\n\n먼저 PyTorch이 제공하는 유용한 기능들을 unit 별로 실습해보는 시간을 갖습니다.\n\n(Tutorial 출처: https://github.com/yunjey/pytorch-tutorial) \n"},{"metadata":{"id":"Q_P1isRA1mVL"},"cell_type":"markdown","source":"### 9.1.1. Autograd\n\n자동으로 미분을 해주는 기능인 autograd(automatic differentiation, AD)을 사용해 봅니다."},{"metadata":{"id":"xvfxxkvC1sUj","trusted":false},"cell_type":"code","source":"# 먼저 tensor 타입의 변수를 선언하고, 계산 그래프를 생성합니다\nx = torch.tensor(1., requires_grad=True)\nw = torch.tensor(2., requires_grad=True)\nb = torch.tensor(3., requires_grad=True)\n\ny = w * x + b    # y = 2 * x + 3\n\n# backward() 함수는 계산 그래프로부터 미분값을 계산합니다.\ny.backward()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ug-8LB7bmXws"},"cell_type":"markdown","source":"이제 자동으로 미분된 결과값을 출력해볼까요?"},{"metadata":{"id":"GPuspmuUmYca","outputId":"a0b8a582-6e0a-40ec-f060-fe388fbcc30d","executionInfo":{"status":"ok","timestamp":1579421561576,"user_tz":-540,"elapsed":4387,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"print(x.grad)    # x.grad = 2 \nprint(w.grad)    # w.grad = 1 \nprint(b.grad)    # b.grad = 1 \n","execution_count":null,"outputs":[]},{"metadata":{"id":"AUsqiyRz13cG"},"cell_type":"markdown","source":"실제 모델 학습 과정에서 autograd이 사용되는 모습은 다음과 같습니다 (다음 코드는 실습해보지 않아도 괜찮습니다)."},{"metadata":{"id":"4GwOoAYo144T","outputId":"d5b4e4ff-d27b-4441-a3b8-eed8eae296a6","executionInfo":{"status":"ok","timestamp":1579421561576,"user_tz":-540,"elapsed":4379,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"# Random한 값으로 데이터를 만듭니다. x의 사이즈는 10x3; y의 사이즈는 10x2 입니다\nx = torch.randn(10, 3)\ny = torch.randn(10, 2)\n\n# x를 입력, y를 출력으로 다룰 수 있는 Linear regression model을 만듭니다\n# (nn 표현으로는 fully connected layer를 생성합니다)\nlinear = nn.Linear(3, 2)\nprint ('w: ', linear.weight)\nprint ('b: ', linear.bias)\n\n# 학습에 사용될 loss function 및 optimizer를 선언 합니다\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n\n# Forward pass\npred = linear(x)\n\n# Compute loss\nloss = criterion(pred, y)\nprint('** loss before 1 step optimization: ', loss.item())\n\n# Backward pass\nloss.backward()\n\n# 계산된 미분값을 출력합니다\nprint ('dL/dw: ', linear.weight.grad) \nprint ('dL/db: ', linear.bias.grad)\n\n# gradient descent의 1 iteration만 실행해 봅니다.\noptimizer.step()\n\n# You can also perform gradient descent at the low level\n# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n\n# gradient descent의 1 iteration 후, 업데이트된 loss 값을 출력합니다.\npred = linear(x)\nloss = criterion(pred, y)\nprint('** loss after 1 step optimization: ', loss.item())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IeMlMGPf135W"},"cell_type":"markdown","source":"### 9.1.2. Loading data from `numpy`\n\n`torch.from_numpy()` 함수를 사용하면 `ndarray` 타입(`numpy`의 다차원 행렬 타입)의 데이터를 torch에서 사용하는 데이터 타입으로 (and *vice versa*) 쉽게 변환 가능합니다."},{"metadata":{"id":"rZ5VD9rt2JNG","trusted":false},"cell_type":"code","source":"import numpy as np\n\n# numpy array를 생성\nx = np.array([[1, 2], [3, 4]])\n\n# numpy array -> torch tensor\ny = torch.from_numpy(x)\n\n# torch tensor -> numpy array\nz = y.numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"_Kt29Jr82KLE"},"cell_type":"markdown","source":"### 9.1.3. DataLoader\n\n`DataLoader`는 학습 및 예측 과정에서 모델에 데이터를 공급해주는 역할을 합니다.\n\n`DataLoader`는 `sklearn.datasets`와 같이 자주 쓰이는 데이터를 내장하고 있으며, 다음 예제를 통해 CIFAR-10이라는 이미지 데이터를 불러오는 작업을 수행해볼 수 있습니다. "},{"metadata":{"id":"dNJvJN9q2Pph","outputId":"5def4438-1e65-43ab-ed79-3aab48bfba9a","executionInfo":{"status":"ok","timestamp":1579421586043,"user_tz":-540,"elapsed":28837,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"# CIFAR-10 데이터셋을 다운로드 및 메모리에 로드\ntrain_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n                                             train=True, \n                                             transform=transforms.ToTensor(),\n                                             download=True)\n\n# 데이터셋의 첫 번째 instance로 접근해보기\nimage, label = train_dataset[0]\nprint (image.size())\nprint (label)\n\n# 데이터셋으로부터 DataLoader 생성\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=64, \n                                           shuffle=True)\n\n# DataLoader로부터 데이터를 받으며 학습하는 code skeleton\nfor images, labels in train_loader:\n    # ----------------------------------------------\n    # -- Your training code should be placed here --\n    # ----------------------------------------------\n    pass\n\n# Iterator를 사용하여 미니배치(mini-batch)를 구현할 수도 있음\n# data_iter = iter(train_loader)\n# images, labels = data_iter.next()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"-Me3bn4I2WJP"},"cell_type":"markdown","source":"다음의 code skeleton을 사용하면, 개별 데이터셋을 torch에서 사용되는 데이터셋 타입으로 불러들일 수 있습니다. 이렇게 작성된 dataset은 DataLoader와 함께 사용될 수 있습니다."},{"metadata":{"id":"5rsCXsGb2YeJ","trusted":false},"cell_type":"code","source":"# 다음 code skeleton을 사용하여 DataLoader를 구성할 수 있습니다\n# (TODO에 해당하는 내용을 채우기 전에는 실행되지 않습니다)\n\n# You should build your custom dataset as below\nclass CustomDataset(torch.utils.data.Dataset):\n  def __init__(self):\n    # TODO\n    # 1. Initialize file paths or a list of file names\n    pass\n  def __getitem__(self, index):\n    # TODO\n    # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open)\n    # 2. Preprocess the data (e.g. torchvision.Transform)\n    # 3. Return a data pair (e.g. image and label)\n    pass\n  def __len__(self):\n    # You should change 0 to the total size of your dataset\n    return 0 \n\n# You can then use the prebuilt data loader\ncustom_dataset = CustomDataset()\ntrain_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n                                           batch_size=64, \n                                           shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"AOZhAiJP2hYQ"},"cell_type":"markdown","source":"### 9.1.4. Loading a Pretrained Model\n\nPyTorch를 통해 작성되고, 학습된 모델(pretrained model)을 파일 형태로 주고 받을 수도 있습니다. 다음 코드는 PyTorch를 통해 제공되는 pretrained ResNet-18 (이미지 분류 모델)을 불러옵니다."},{"metadata":{"id":"LxB1NUPj2mWl","outputId":"705f74a7-ea89-4608-ecb5-418d41176890","executionInfo":{"status":"ok","timestamp":1579421604961,"user_tz":-540,"elapsed":7941,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"# Pretrained ResNet-18 다운로드 및 불러오기\nresnet = torchvision.models.resnet18(pretrained=True)\n\n\n# 다운로드 받은 pretrained model을 추가로 더 학습하는 것도 가능합니다\n# 다음 코드는 ResNet-18의 가장 마지말 레이어만 추가 학습하는 예시를 보여줍니다\nfor param in resnet.parameters():\n    param.requires_grad = False\nresnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example\n\n\n# Forward pass를 통해 pretrained model을 예측 작업에 바로 적용해볼 수 있습니다\nimages = torch.randn(64, 3, 224, 224)\noutputs = resnet(images)\nprint (outputs.size())     # (64, 100)","execution_count":null,"outputs":[]},{"metadata":{"id":"xAu0RY1zyr8m"},"cell_type":"markdown","source":"## 9.2. Logistic Regression (revisited)\n\n이번 절에서는 우리가 이미 알고있는 logistic regression 모델을 PyTorch로 구현해보는 과정을 통해, scikit-learn (sklearn)과는 다른 PyTorch 용법을 실습해 봅니다."},{"metadata":{"id":"aNlGXeNVy3-2"},"cell_type":"markdown","source":"실습에 사용할 데이터를 불러와 DataLoader를 구성합니다. 사용할 데이터는 손글씨 숫자 이미지를 담고있는 MNIST 데이터셋 입니다. \n\n![Image is not found](https://miro.medium.com/max/530/1*VAjYygFUinnygIx9eVCrQQ.png)"},{"metadata":{"id":"OsJos21Py6PQ","outputId":"d2de09aa-3502-4bce-818a-7a2f87585f96","executionInfo":{"status":"ok","timestamp":1579442459999,"user_tz":-540,"elapsed":3458,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":true},"cell_type":"code","source":"input_size = 784\nnum_classes = 10\nbatch_size = 100\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.MNIST(root='../../data', \n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dcd27652f51470a855d36435c8e4ad3"}},"metadata":{}},{"output_type":"error","ename":"URLError","evalue":"<urlopen error [Errno -3] Temporary failure in name resolution>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1319\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1320\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    937\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 938\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    939\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9e8e5ce80e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                            \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                            \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                            download=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m test_dataset = torchvision.datasets.MNIST(root='../../data', \n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 )\n\u001b[1;32m     82\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;31m# check integrity of downloaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     69\u001b[0m             urllib.request.urlretrieve(\n\u001b[1;32m     70\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"]}]},{"metadata":{"id":"BBB2Vq9Rz2Mj"},"cell_type":"markdown","source":"PyTorch로 logistic regression 모델을 정의하는 과정은 다음과 같습니다."},{"metadata":{"id":"E0kvRCEtzsIB","trusted":true},"cell_type":"code","source":"# 실험에 사용할 하이퍼파라메터들을 설정 합니다\nlearning_rate = 0.001\n\n# 하나의 linear 레이어로 구성된 네트워크를 만듭니다\nmodel = nn.Linear(input_size, num_classes)\n\n# 모델이 사용할 loss function(i.e., objective function)과 optimizer를 지정합니다\n# nn.CrossEntropyLoss() computes softmax internally\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","execution_count":3,"outputs":[]},{"metadata":{"id":"uq6lkpBHzNkp"},"cell_type":"markdown","source":"위 모델을 위한 학습 및 예측 함수를 정의합니다."},{"metadata":{"id":"Z-fmSgOlzPd9","trusted":true},"cell_type":"code","source":"# Train the model\ndef train_logreg(train_loader, num_epochs):\n  total_step = len(train_loader)\n  for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n      # Reshape images to (batch_size, input_size)\n      images = images.reshape(-1, 28*28)\n      \n      # Forward pass\n      outputs = model(images)\n      loss = criterion(outputs, labels)\n      \n      # Backward and optimize\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      \n      # Display the progress\n      if (i+1) % 300 == 0:\n        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n              .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n        \n# Test the model\ndef test_logreg(model, test_loader):\n  # In test phase, we don't need to compute gradients (for memory efficiency)\n  with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n      images = images.reshape(-1, 28*28)\n      outputs = model(images)\n      _, predicted = torch.max(outputs.data, 1)\n      total += labels.size(0)\n      correct += (predicted == labels).sum()\n\n    # Display the result\n    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))","execution_count":4,"outputs":[]},{"metadata":{"id":"YUBETKfrzpPR"},"cell_type":"markdown","source":"위에서 정의한 함수들을 사용하여 logistic regression 모델을 학습/평가해 봅니다. \n\n얼마나 높은 accuracy를 얻을 수 있나요?"},{"metadata":{"id":"jIvwcPfuu60P","outputId":"0b84f463-c0c5-4da0-eea5-17e591f10a11","executionInfo":{"status":"ok","timestamp":1579421662092,"user_tz":-540,"elapsed":56755,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":true},"cell_type":"code","source":"tr_start=time.time()\ntrain_logreg(train_loader, num_epochs=10)\nprint(\"train time: \", time.time()-tr_start)\n\nts_start=time.time()\ntest_logreg(model, test_loader)\nprint(\"test time: \", time.time()-ts_start)","execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'time' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ab55e95a688e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_logreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train time: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtr_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mts_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"]}]},{"metadata":{"id":"E8VYcSa8yUBs"},"cell_type":"markdown","source":"## 9.3. Feedforward Neural Networks\n\n이번 절에서는 가장 기초적인 neural network이라고 할 수 있는 Feedforward Neural Network (multi-layer perceptron, MLP)을 작성하고 MNIST 데이터를 학습하는 데에 적용해보겠습니다."},{"metadata":{"id":"1x4w2RrxvOOf"},"cell_type":"markdown","source":"먼저, 실험에 사용할 하이퍼파라메터들을 설정 합니다."},{"metadata":{"id":"cTMN2RwEvRrv","trusted":false},"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \nhidden_size = 500\nlearning_rate = 0.001","execution_count":null,"outputs":[]},{"metadata":{"id":"D5PH0aZ5vEyv"},"cell_type":"markdown","source":"다음 class definition을 통해, 이번 절에서 사용할 FFNet 모델을 정의합니다."},{"metadata":{"id":"rFIIFo6cvE-a","trusted":false},"cell_type":"code","source":"# Fully connected neural network with one hidden layer\nclass FFNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(FFNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"id":"52e0kDSNw66t"},"cell_type":"markdown","source":"위 모델을 위한 학습 및 예측 함수를 정의합니다."},{"metadata":{"id":"S5mC1TZfw8or","trusted":false},"cell_type":"code","source":"# Train the model\ndef train_ffnet(model, train_loader, num_epochs):\n  total_step = len(train_loader)\n  for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n      # Move tensors to the configured device\n      images = images.reshape(-1, 28*28).to(device)\n      labels = labels.to(device)\n      \n      # Forward pass\n      outputs = model(images)\n      loss = criterion(outputs, labels)\n      \n      # Backward and optimize\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      \n      # Display the progress\n      if (i+1) % 300 == 0:\n        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n        \n# Test the model\ndef test_ffnet(model, test_loader):\n  # In test phase, we don't need to compute gradients (for memory efficiency)\n  with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n      images = images.reshape(-1, 28*28).to(device)\n      labels = labels.to(device)\n      outputs = model(images)\n      _, predicted = torch.max(outputs.data, 1)\n      total += labels.size(0)\n      correct += (predicted == labels).sum().item()\n\n    # Display the result\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"oYJFsJgPvWxt"},"cell_type":"markdown","source":"위에서 정의한 모델과 함수들을 사용하여 MNIST 데이터에 적용해 봅니다. \n\nFFNet을 사용하여 얼마나 높은 accuracy를 얻을 수 있나요?"},{"metadata":{"id":"WEm48nArvYi3","outputId":"52db519b-e457-4e1a-c0d1-26eb877a6c88","executionInfo":{"status":"ok","timestamp":1579442585316,"user_tz":-540,"elapsed":97690,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"# declare a model object (instantiation)\nmodel = FFNet(input_size, hidden_size, num_classes).to(device)\n\n# Set the loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\ntrain_ffnet(model, train_loader, num_epochs=10)\ntest_ffnet(model, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"id":"NIMqn7690Zfa"},"cell_type":"markdown","source":"## 9.4. Convolutional Neural Networks"},{"metadata":{"id":"1OtUysrF0fZO"},"cell_type":"markdown","source":"이번 절에서는 이미지 학습/예측에 적합한 neural network의 한 종류인 Convolutional Neural Network (CNN)을 작성하고 MNIST 데이터에 적용해보겠습니다.\n\n가장 먼저, 실험에 사용할 하이퍼파라메터들을 설정 합니다."},{"metadata":{"id":"wXa_HLjl45Hy","trusted":false},"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \nlearning_rate = 0.001","execution_count":null,"outputs":[]},{"metadata":{"id":"_tE26cVz6z7I"},"cell_type":"markdown","source":"다음 class definition을 통해, 이번 절에서 사용할 CNN 모델을 정의합니다."},{"metadata":{"id":"rQqZcoH90g3S","trusted":false},"cell_type":"code","source":"# Convolutional neural network (two convolutional layers)\nclass ConvNet(nn.Module):\n  def __init__(self, num_classes=10):\n    super(ConvNet, self).__init__()\n    self.layer1 = nn.Sequential(\n      nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n      nn.BatchNorm2d(16),\n      nn.ReLU(),\n      nn.MaxPool2d(kernel_size=2, stride=2))\n    self.layer2 = nn.Sequential(\n      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n      nn.BatchNorm2d(32),\n      nn.ReLU(),\n      nn.MaxPool2d(kernel_size=2, stride=2))\n    self.fc = nn.Linear(7*7*32, num_classes)\n      \n  def forward(self, x):\n    out = self.layer1(x)\n    out = self.layer2(out)\n    out = out.reshape(out.size(0), -1)\n    out = self.fc(out)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"id":"OEUrl-Fb0k3j"},"cell_type":"markdown","source":"위 모델을 위한 학습 및 예측 함수를 정의합니다."},{"metadata":{"id":"hTancuuf0nKQ","trusted":false},"cell_type":"code","source":"# Train the model\ndef train_convnet(train_loader, num_epochs):\n  total_step = len(train_loader)\n  for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n      images = images.to(device)\n      labels = labels.to(device)\n      \n      # Forward pass\n      outputs = model(images)\n      loss = criterion(outputs, labels)\n      \n      # Backward and optimize\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      \n      # Display the progress\n      if (i+1) % 300 == 0:\n        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n              .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model      \ndef test_convnet(model, test_loader):\n  model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n  with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n      images = images.to(device)\n      labels = labels.to(device)\n      outputs = model(images)\n      _, predicted = torch.max(outputs.data, 1)\n      total += labels.size(0)\n      correct += (predicted == labels).sum().item()\n\n    # Display the result\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))","execution_count":null,"outputs":[]},{"metadata":{"id":"9rWNjW5l0-Jg"},"cell_type":"markdown","source":"위에서 정의한 모델과 함수들을 사용하여 MNIST 데이터에 적용해 봅니다. \n\nCNN을 사용하여 얼마나 높은 accuracy를 얻을 수 있나요?"},{"metadata":{"id":"zb6KataZ09I9","outputId":"c0e1ad96-8420-414f-debc-50a16beb726c","executionInfo":{"status":"ok","timestamp":1579422369709,"user_tz":-540,"elapsed":707553,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"model = ConvNet(num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\ntr_start=time.time()\ntrain_convnet(train_loader, num_epochs=10)\nprint(\"train time: \", time.time()-tr_start)\n\nts_start=time.time()\ntest_convnet(model, test_loader)\nprint(\"test time: \", time.time()-ts_start)","execution_count":null,"outputs":[]},{"metadata":{"id":"WsOgNgDS23sU"},"cell_type":"markdown","source":"## 9.5. Skorch (Optional)\n\nSkorch는 PyTorch를 scikit-learn (sklearn) 라이브러리와 함께 사용할 수 있게 해주는 wrapper 라이브러리 입니다. Skorch를 사용하면, sklearn에서 유용하게 사용되는 pipeline 구조(이번 캠프에서 다루지는 않았습니다)와 GridSearchCV를 PyTorch와도 사용할 수 있게 됩니다.\n\n먼저 해당 라이브러리를 설치 합니다."},{"metadata":{"id":"nHtngGhH3Fzd","outputId":"71d44afb-7e9f-465d-d2e3-60bd454c05ca","executionInfo":{"status":"ok","timestamp":1579442599305,"user_tz":-540,"elapsed":7136,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"!pip install skorch","execution_count":null,"outputs":[]},{"metadata":{"id":"JICZaWEPIqxq"},"cell_type":"markdown","source":"이제 sklearn이 처리할 수 있는 타입으로 데이터셋을 다시 준비합니다."},{"metadata":{"id":"Wda0lP1xAHdQ","outputId":"1bb3ee32-5ee1-4c57-bce2-8ccc7a7fdd78","executionInfo":{"status":"ok","timestamp":1579443808254,"user_tz":-540,"elapsed":13020,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"# MNIST dataset \ntrain_dataset = torchvision.datasets.MNIST(root='../../data', \n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=len(train_dataset), \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=len(test_dataset), \n                                          shuffle=False)\n\n# numpy array\nX_tr = next(iter(train_loader))[0].numpy()\ny_tr = next(iter(train_loader))[1].numpy()\nX_ts = next(iter(test_loader))[0].numpy()\ny_ts = next(iter(test_loader))[1].numpy()\n\nX_tr = X_tr.reshape(-1, input_size)\nX_ts = X_ts.reshape(-1, input_size)\n\nX_ts = X_ts / X_tr.max()\nX_tr = X_tr / X_tr.max()\nprint(X_tr.shape)\nprint(y_tr.shape)\nprint(X_ts.shape)\nprint(y_ts.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"Vkt-pIu83bGf"},"cell_type":"markdown","source":"이제 GridSearchCV framework을 통해, FFNet 모델을 학습해 봅니다. 즉, 다음 코드를 통해 다음 하이퍼파라메터 중 최적의 조합을 찾습니다.\n - 'lr': [0.01, 0.001, 0.0001]\n - 'max_epochs': [5, 10, 15]\n "},{"metadata":{"id":"25ZDLxwQ-T4s","outputId":"5502c489-7ef9-49be-f6e6-d91e3b27f914","executionInfo":{"status":"ok","timestamp":1579443998430,"user_tz":-540,"elapsed":182645,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"from skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# from sklearn.metrics import make_scorer\n# from skorch.callbacks import EpochScoring\n\n# def accuracy_argmax(y_true, y_pred):\n#   return np.mean(y_true == np.argmax(y_pred, -1))\n\n# accuracy_argmax_scorer = make_scorer(accuracy_argmax)\n\n# epoch_acc = EpochScoring(\n#     accuracy_argmax_scorer,\n#     name='valid_acc',\n#     lower_is_better=False\n# )\n\n# hyper-parameters\ninput_size = 784\nnum_classes = 10\nbatch_size = 100\nhidden_size = 500\n\nnet = NeuralNetClassifier(FFNet, \n                          batch_size = 100,\n                          criterion = nn.CrossEntropyLoss,\n                          optimizer = torch.optim.Adam,\n                          # callbacks=[epoch_acc],\n                          iterator_train__shuffle=True)\n\nparams = {\n    'lr': [0.001], #[0.01, 0.001, 0.0001],\n    'max_epochs': [10], #[5, 10, 15],\n    'module__input_size': [input_size],\n    'module__hidden_size': [hidden_size],\n    'module__num_classes': [num_classes]\n}\ngridsearch = GridSearchCV(net, params, cv=3, scoring='accuracy')\n\n\ngridsearch.fit(X_tr, y_tr)\nprint(gridsearch.best_score_, gridsearch.best_params_)\n\nbest_net = gridsearch.best_estimator_\ny_pred = best_net.predict(X_ts)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"SwwqGiyq27wK","outputId":"95ab3bfe-5cb5-4c96-acdc-66708809acad","executionInfo":{"status":"ok","timestamp":1579444014902,"user_tz":-540,"elapsed":1551,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"trusted":false},"cell_type":"code","source":"print(y_pred[0:10])\nprint(y_ts[0:10])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"xCk6W1vS36pz"},"cell_type":"markdown","source":"## References\n* yunjey. PyTorch Tutorial for Deep Learning Researchers. URL: https://github.com/yunjey/pytorch-tutorial\n* Skorch Tutorials. URL: https://skorch.readthedocs.io/en/stable/user/tutorials.html"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}